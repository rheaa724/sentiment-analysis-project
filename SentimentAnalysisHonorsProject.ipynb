{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e8e54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: scikit-learn in ./anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: nltk in ./anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#installing packages + libraries (contain necessary functions) necessary to manipulate the dataset\n",
    "# pandas + numpy to manipulate dataset\n",
    "# Scikit-learn for splitting data into testing + training sets\n",
    "! pip install tensorflow scikit-learn pandas numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3fb32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e244d99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>file</th>\n",
       "      <th>cool</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>business_id</th>\n",
       "      <th>funny</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>useful</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentiyelp/Charlotte_North_Carolina</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>BLfLwh7w4NGHU7eBLgnaFg</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ</td>\n",
       "      <td>0</td>\n",
       "      <td>SO GOOD!!! I did not dine in I ordered take ou...</td>\n",
       "      <td>YmAqQyaFli8H9LVGhGhR9w</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentiyelp/Charlotte_North_Carolina</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>X6-KGabYKJTS1Dsipo4XIw</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-05-08</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ</td>\n",
       "      <td>0</td>\n",
       "      <td>The food is tasty, no doubt, but rotisserie ch...</td>\n",
       "      <td>ciXjBfJrAEteIKpzZg4I9g</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentiyelp/Charlotte_North_Carolina</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>b_OaEC8uyqIUN-BtX6KNQA</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Pio Pio was recommended by a friend a few mont...</td>\n",
       "      <td>17qZxRhTcp_JSnEg65COkA</td>\n",
       "      <td>3</td>\n",
       "      <td>0.950852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentiyelp/Charlotte_North_Carolina</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>QBe4o5-8NNzDuHkM7Jc05A</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ</td>\n",
       "      <td>0</td>\n",
       "      <td>I didnt hate Pio Pio, but I didnt like it enou...</td>\n",
       "      <td>gc4WUy07eaQYPGOly1t-Ww</td>\n",
       "      <td>1</td>\n",
       "      <td>0.123886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentiyelp/Charlotte_North_Carolina</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>I_Fm3v1N9nrWqNqTS80ZMQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>-cZ6Hhc9F7VkKXxHMVZSQ</td>\n",
       "      <td>0</td>\n",
       "      <td>I returned to Pio Pio with my family on a Frid...</td>\n",
       "      <td>EHq8HvrDG-VaJ8rjyjVUYQ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.790467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  dir                       file  cool  \\\n",
       "0  sentiyelp/Charlotte_North_Carolina  -cZ6Hhc9F7VkKXxHMVZSQ.csv     0   \n",
       "1  sentiyelp/Charlotte_North_Carolina  -cZ6Hhc9F7VkKXxHMVZSQ.csv     0   \n",
       "2  sentiyelp/Charlotte_North_Carolina  -cZ6Hhc9F7VkKXxHMVZSQ.csv     1   \n",
       "3  sentiyelp/Charlotte_North_Carolina  -cZ6Hhc9F7VkKXxHMVZSQ.csv     0   \n",
       "4  sentiyelp/Charlotte_North_Carolina  -cZ6Hhc9F7VkKXxHMVZSQ.csv     0   \n",
       "\n",
       "                review_id  stars        date            business_id  funny  \\\n",
       "0  BLfLwh7w4NGHU7eBLgnaFg      4  2013-04-28  -cZ6Hhc9F7VkKXxHMVZSQ      0   \n",
       "1  X6-KGabYKJTS1Dsipo4XIw      3  2013-05-08  -cZ6Hhc9F7VkKXxHMVZSQ      0   \n",
       "2  b_OaEC8uyqIUN-BtX6KNQA      4  2013-05-11  -cZ6Hhc9F7VkKXxHMVZSQ      1   \n",
       "3  QBe4o5-8NNzDuHkM7Jc05A      2  2013-05-18  -cZ6Hhc9F7VkKXxHMVZSQ      0   \n",
       "4  I_Fm3v1N9nrWqNqTS80ZMQ      5  2013-05-18  -cZ6Hhc9F7VkKXxHMVZSQ      0   \n",
       "\n",
       "                                                text                 user_id  \\\n",
       "0  SO GOOD!!! I did not dine in I ordered take ou...  YmAqQyaFli8H9LVGhGhR9w   \n",
       "1  The food is tasty, no doubt, but rotisserie ch...  ciXjBfJrAEteIKpzZg4I9g   \n",
       "2  Pio Pio was recommended by a friend a few mont...  17qZxRhTcp_JSnEg65COkA   \n",
       "3  I didnt hate Pio Pio, but I didnt like it enou...  gc4WUy07eaQYPGOly1t-Ww   \n",
       "4  I returned to Pio Pio with my family on a Frid...  EHq8HvrDG-VaJ8rjyjVUYQ   \n",
       "\n",
       "   useful     score  \n",
       "0       0  0.970178  \n",
       "1       1  0.782714  \n",
       "2       3  0.950852  \n",
       "3       1  0.123886  \n",
       "4       1  0.790467  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"yelp_1000.csv\")\n",
    "#this is just a larger version of the dataset df = pd.read_csv(\"https://github.com/justmarkham/DAT7/raw/master/data/yelp.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c3ac84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SO GOOD!!! I did not dine in I ordered take ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The food is tasty, no doubt, but rotisserie ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pio Pio was recommended by a friend a few mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didnt hate Pio Pio, but I didnt like it enou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I returned to Pio Pio with my family on a Frid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  SO GOOD!!! I did not dine in I ordered take ou...\n",
       "1  The food is tasty, no doubt, but rotisserie ch...\n",
       "2  Pio Pio was recommended by a friend a few mont...\n",
       "3  I didnt hate Pio Pio, but I didnt like it enou...\n",
       "4  I returned to Pio Pio with my family on a Frid..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Extraneous Columns\n",
    "remove_columns = ['dir', 'file', 'cool', 'review_id', 'stars','date','business_id', 'funny', 'user_id', 'useful', 'score']\n",
    "df = df.drop(remove_columns, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4560d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good dine ordered take took little longer expected get food well worth got home order roasted chicken rice beans fried yucca boom delicious could something simple good pio pio knows everything full flavor portion size typical top like spanish restaurants price paying worth back\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rheaagrawal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPROCESSING\n",
    "# this tokenizes the data then joins the string back together\n",
    "# making data in 'text' column lowercase\n",
    "df['lower'] = df['text'].str.lower()\n",
    "# Use regex substitution to remove punctuation from 'lower' column\n",
    "import re\n",
    "pattern = r'[^\\w\\s]'\n",
    "df['no_punctuation'] = df['lower'].apply(lambda x: re.sub(pattern, '', x))\n",
    "# removing stopwords from 'no_punctuation' column\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #nltk is a massive nlp dataset that already has a usable list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "df['cleaned'] = df['no_punctuation'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "# df.head()\n",
    "# print(df.loc[0:5,'text'])\n",
    "print(df.loc[0,'cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646ce296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rheaagrawal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good dine ordered take took little longer expected get food well worth got home order roasted chicken rice bean fried yucca boom delicious could something simple good pio pio know everything full flavor portion size typical top like spanish restaurant price paying worth back\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['cleaned'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "# df.iloc[:10]\n",
    "print(df.loc[0,'lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a89d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good dine order take took littl longer expect get food well worth got home order roast chicken rice bean fri yucca boom delici could someth simpl good pio pio know everyth full flavor portion size typic top like spanish restaur price pay worth back\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['cleaned'].apply(lambda x: ' '.join(stemmer.stem(word) for word in x.split()))\n",
    "# df.head()\n",
    "print(df.loc[0,'stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30af8a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rheaagrawal/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punctuation</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SO GOOD!!! I did not dine in I ordered take ou...</td>\n",
       "      <td>so good!!! i did not dine in i ordered take ou...</td>\n",
       "      <td>so good i did not dine in i ordered take out i...</td>\n",
       "      <td>good dine ordered take took little longer expe...</td>\n",
       "      <td>good dine ordered take took little longer expe...</td>\n",
       "      <td>good dine order take took littl longer expect ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The food is tasty, no doubt, but rotisserie ch...</td>\n",
       "      <td>the food is tasty, no doubt, but rotisserie ch...</td>\n",
       "      <td>the food is tasty no doubt but rotisserie chic...</td>\n",
       "      <td>food tasty doubt rotisserie chicken similar ev...</td>\n",
       "      <td>food tasty doubt rotisserie chicken similar ev...</td>\n",
       "      <td>food tasti doubt rotisseri chicken similar eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pio Pio was recommended by a friend a few mont...</td>\n",
       "      <td>pio pio was recommended by a friend a few mont...</td>\n",
       "      <td>pio pio was recommended by a friend a few mont...</td>\n",
       "      <td>pio pio recommended friend months ago group us...</td>\n",
       "      <td>pio pio recommended friend month ago group u w...</td>\n",
       "      <td>pio pio recommend friend month ago group us we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didnt hate Pio Pio, but I didnt like it enou...</td>\n",
       "      <td>i didnt hate pio pio, but i didnt like it enou...</td>\n",
       "      <td>i didnt hate pio pio but i didnt like it enoug...</td>\n",
       "      <td>didnt hate pio pio didnt like enough go back e...</td>\n",
       "      <td>didnt hate pio pio didnt like enough go back e...</td>\n",
       "      <td>didnt hate pio pio didnt like enough go back e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I returned to Pio Pio with my family on a Frid...</td>\n",
       "      <td>i returned to pio pio with my family on a frid...</td>\n",
       "      <td>i returned to pio pio with my family on a frid...</td>\n",
       "      <td>returned pio pio family friday night see food ...</td>\n",
       "      <td>returned pio pio family friday night see food ...</td>\n",
       "      <td>return pio pio famili friday night see food wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  SO GOOD!!! I did not dine in I ordered take ou...   \n",
       "1  The food is tasty, no doubt, but rotisserie ch...   \n",
       "2  Pio Pio was recommended by a friend a few mont...   \n",
       "3  I didnt hate Pio Pio, but I didnt like it enou...   \n",
       "4  I returned to Pio Pio with my family on a Frid...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  so good!!! i did not dine in i ordered take ou...   \n",
       "1  the food is tasty, no doubt, but rotisserie ch...   \n",
       "2  pio pio was recommended by a friend a few mont...   \n",
       "3  i didnt hate pio pio, but i didnt like it enou...   \n",
       "4  i returned to pio pio with my family on a frid...   \n",
       "\n",
       "                                      no_punctuation  \\\n",
       "0  so good i did not dine in i ordered take out i...   \n",
       "1  the food is tasty no doubt but rotisserie chic...   \n",
       "2  pio pio was recommended by a friend a few mont...   \n",
       "3  i didnt hate pio pio but i didnt like it enoug...   \n",
       "4  i returned to pio pio with my family on a frid...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  good dine ordered take took little longer expe...   \n",
       "1  food tasty doubt rotisserie chicken similar ev...   \n",
       "2  pio pio recommended friend months ago group us...   \n",
       "3  didnt hate pio pio didnt like enough go back e...   \n",
       "4  returned pio pio family friday night see food ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  good dine ordered take took little longer expe...   \n",
       "1  food tasty doubt rotisserie chicken similar ev...   \n",
       "2  pio pio recommended friend month ago group u w...   \n",
       "3  didnt hate pio pio didnt like enough go back e...   \n",
       "4  returned pio pio family friday night see food ...   \n",
       "\n",
       "                                             stemmed sentiment  \n",
       "0  good dine order take took littl longer expect ...         1  \n",
       "1  food tasti doubt rotisseri chicken similar eve...         1  \n",
       "2  pio pio recommend friend month ago group us we...         1  \n",
       "3  didnt hate pio pio didnt like enough go back e...         1  \n",
       "4  return pio pio famili friday night see food wo...         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentiment Analysis with Vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def calculate_sentiment(review): \n",
    "    polarity = analyzer.polarity_scores(review)\n",
    "#     print(review)\n",
    "#    print(polarity)\n",
    "    #positive sentiment\n",
    "    if polarity['compound']>=0.05:\n",
    "        return \"1\"\n",
    "    #negative sentiment\n",
    "    elif polarity['compound']<=-0.05:\n",
    "        return \"-1\"\n",
    "    #neutral sentiment\n",
    "    elif polarity['compound']>-0.05 and polarity['compound']<0.05:\n",
    "        return \"0\"\n",
    "\n",
    "df['sentiment'] = df['stemmed'].apply(calculate_sentiment)\n",
    "#print(df.loc[0:50, 'sentiment'])\n",
    "#print(df.loc[19,['text','sentiment']])\n",
    "#print(df.loc[40, ['text', 'stemmed','sentiment']])\n",
    "# df.iloc[0:50]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55ffacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935\n"
     ]
    }
   ],
   "source": [
    "#Creating the Training Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "X = tfidf.fit_transform(df['stemmed'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "Y = df['sentiment']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticR = LogisticRegression(solver=\"liblinear\")\n",
    "logisticR.fit(X_train, Y_train)\n",
    "predictions = logisticR.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(predictions, Y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
